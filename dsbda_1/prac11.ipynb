{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3eb80a89-de50-4d21-bf8d-ce99ee17b1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Everyone', ',', 'How', 'are', 'you', '?', 'Now', ',', 'we', 'will', 'see', 'practical', 'number', '07', '.']\n",
      "['We', 'learn', 'C', ',', 'C++', 'and', 'Python', 'Language']\n",
      "work\n",
      "work\n",
      "work\n",
      "work\n",
      "commun\n",
      "danc\n",
      "sing\n",
      "play\n",
      "sing\n",
      "play\n",
      "hardwork\n",
      "Communication\n",
      "worker\n",
      "beech\n",
      "['i', 'am', 'student', 'of', 'computer', 'engineering', 'department']\n",
      "['student', 'computer', 'engineering', 'department']\n",
      "['Hello Everyone, How are you?', 'Now, we will see practical number 07.']\n",
      "['Let', '’', 's', 'lemmatize', 'a', 'simple', 'sentence', '.', 'We', 'first', 'tokenize', 'the', 'sentence', 'into', 'words', 'using', 'nltk', '.']\n",
      "Number of words in the corpus: 14\n",
      "The words in the corpus:\n",
      " {'the', 'scientists', 'important', 'best', 'of', 'science', 'most', 'data', 'courses', 'is', 'one', 'analyze', 'fields', 'this'}\n",
      "\n",
      "TF DataFrame:\n",
      "     python     great        is       for   science   we      data  love  and  \\\n",
      "0  0.200000  0.000000  0.000000  0.000000  0.000000  0.2  0.200000   0.2  0.2   \n",
      "1  0.166667  0.166667  0.166667  0.166667  0.166667  0.0  0.166667   0.0  0.0   \n",
      "2  0.250000  0.000000  0.000000  0.000000  0.250000  0.0  0.250000   0.0  0.0   \n",
      "\n",
      "   loves  \n",
      "0   0.00  \n",
      "1   0.00  \n",
      "2   0.25  \n",
      "\n",
      "IDF values:\n",
      "         python:     0.0000\n",
      "          great:     0.4771\n",
      "             is:     0.4771\n",
      "            for:     0.4771\n",
      "        science:     0.1761\n",
      "             we:     0.4771\n",
      "           data:     0.0000\n",
      "           love:     0.4771\n",
      "            and:     0.4771\n",
      "          loves:     0.4771\n",
      "\n",
      "TF-IDF DataFrame:\n",
      "   python    great       is      for   science        we  data      love  \\\n",
      "0     0.0  0.00000  0.00000  0.00000  0.000000  0.095424   0.0  0.095424   \n",
      "1     0.0  0.07952  0.07952  0.07952  0.029349  0.000000   0.0  0.000000   \n",
      "2     0.0  0.00000  0.00000  0.00000  0.044023  0.000000   0.0  0.000000   \n",
      "\n",
      "        and    loves  \n",
      "0  0.095424  0.00000  \n",
      "1  0.000000  0.00000  \n",
      "2  0.000000  0.11928  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenization example\n",
    "text = \"Hello Everyone, How are you? Now, we will see practical number 07.\"\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# Another tokenization example\n",
    "s = \"We learn C, C++ and Python Language\"\n",
    "tokens = word_tokenize(s)\n",
    "print(tokens)\n",
    "\n",
    "# Porter Stemming examples\n",
    "porter = PorterStemmer()\n",
    "print(porter.stem(\"work\"))\n",
    "print(porter.stem(\"working\"))\n",
    "print(porter.stem(\"works\"))\n",
    "print(porter.stem(\"worked\"))\n",
    "print(porter.stem(\"Communication\"))\n",
    "print(porter.stem(\"Dancing\"))\n",
    "print(porter.stem(\"Sing\"))\n",
    "print(porter.stem(\"Playing\"))\n",
    "print(porter.stem(\"Sings\"))\n",
    "print(porter.stem(\"played\"))\n",
    "print(porter.stem(\"Hardworking\"))\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"Communication\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"workers\"))\n",
    "print(lemmatizer.lemmatize(\"beeches\"))\n",
    "\n",
    "text2 = \"I am student of Computer engineering Department\"\n",
    "text2 = text2.lower()\n",
    "result = word_tokenize(text2)\n",
    "print(result)\n",
    "\n",
    "# Stopwords removal\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in result if word not in stop_words]\n",
    "print(filtered_words)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    "\n",
    "# Lemmatization on tokenized sentence\n",
    "text3 = \"Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.\"\n",
    "word_list = word_tokenize(text3)\n",
    "print(word_list)\n",
    "\n",
    "# Word set creation from corpus\n",
    "corpus = [\n",
    "    'data science is one of the most important fields of science',\n",
    "    'this is one of the best data science courses',\n",
    "    'data scientists analyze data'\n",
    "]\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split()\n",
    "    words_set.update(words)\n",
    "print('Number of words in the corpus:', len(words_set))\n",
    "print('The words in the corpus:\\n', words_set)\n",
    "\n",
    "# TF calculation\n",
    "corpus = [\n",
    "    \"we love python and data\",\n",
    "    \"python is great for data science\",\n",
    "    \"data science loves python\"\n",
    "]\n",
    "\n",
    "words_set = set(\" \".join(corpus).split())\n",
    "n_docs = len(corpus)\n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=list(words_set))\n",
    "\n",
    "# Compute TF\n",
    "for i in range(n_docs):\n",
    "    words = corpus[i].split()\n",
    "    for w in words:\n",
    "        df_tf.loc[i, w] += 1 / len(words)\n",
    "\n",
    "print(\"\\nTF DataFrame:\")\n",
    "print(df_tf)\n",
    "\n",
    "# Compute IDF\n",
    "idf = {}\n",
    "for w in words_set:\n",
    "    k = sum(1 for doc in corpus if w in doc.split())\n",
    "    idf[w] = np.log10(n_docs / k)\n",
    "\n",
    "print(\"\\nIDF values:\")\n",
    "for w in words_set:\n",
    "    print(f'{w:>15}: {idf[w]:>10.4f}')\n",
    "\n",
    "# Compute TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf.loc[i, w] *= idf[w]\n",
    "\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "print(df_tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a502499-3e8c-4571-81fd-e0294f079e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
